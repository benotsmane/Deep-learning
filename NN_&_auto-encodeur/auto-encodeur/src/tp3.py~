from pathlib import Path
import os
import torch
from torchvision.utils import make_grid
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import datetime
from datamaestro import prepare_dataset
import matplotlib.pyplot as plt


#images_target = torch.tensor(test_images[:8]).unsqueeze(1).repeat(1,3,1,1).double()/255.
# Permet de fabriquer une grille d'images
#images = make_grid(images_target)
# Affichage avec tensorboard
#writer.add_image(f'Image originale', images, 0)

    

# Tensorboard : rappel, lancer dans une console tensorboard --logdir runs
writer = SummaryWriter("runs1/V2_BCE_500_1/runs"+datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))



savepath = Path("model.pch")

#  TODO: 




class MonDataset(Dataset):
    def __init__(self, X, Y):
        self.X = torch.flatten(torch.tensor(X/255), start_dim=1).type(torch.FloatTensor)  #On normalise nos images entre 0 et 1 par le max couleur
        
        self.Y = Y


    def __getitem__(self, index):
        return self.X[index], self.Y[index]


    def __len__(self):
        return len(self.X)






##Construction de l'auto-encodeur

class Autoencodeur(nn.Module):

    def __init__(self, dim_input, dim_output):
        super().__init__()
        self.w = nn.Parameter(torch.randn(dim_input, dim_output))
        self.b1 = nn.Parameter(torch.randn(dim_output))
        self.b2 = nn.Parameter(torch.randn(dim_input))
        
    
    def encode(self, X):
        return F.relu(F.linear(X, torch.transpose(self.w,1,0), self.b1))

    def decode(self, encoded_X):
        return F.sigmoid(F.linear(encoded_X, self.w, self.b2))

    def forward(self, X):
        return self.decode(self.encode(X))


###


class State:
    def __init__(self, model, optim ):
        self.model = model
        self.optim = optim
        self.epoch = 0
        self.iteration = 0
        
     
def descente_gradient(device, model, data_train, data_test, epsilon=0.1, epoch=1000):
    #parametre à optimiser
    optim = torch.optim.SGD(model.parameters(), lr=epsilon)
    #MSE loss
    #loss_func = torch.nn.MSELoss()

    #binary cross entropy loss
    loss_func = torch.nn.BCELoss()

    rec_loss_train = [None]*epoch   #record train loss
    rec_loss_test = [None]*epoch

    #checkpoint
    if savepath.is_file():
        with savepath.open("rb") as fp:
            state = torch.load(fp)

    else:
        state = State(model, optim)

    for n_iter in range(state.epoch, epoch):
        cumul_loss = 0
        count_batch = 0
        for X_batch, _ in data_train:
            #Reinitialisation du gradient
            state.optim.zero_grad()
            #Calculer les images autoencoder
            X_batch = X_batch.to(device)
            X_hat = state.model.forward(X_batch)

            loss = loss_func(X_hat, X_batch)

            loss.backward()

            #Mise à jour paramétres du modéle
            state.optim.step()
            state.iteration +=1

            cumul_loss +=loss
            count_batch +=1


        with savepath.open("wb") as fp:
            state.epoch = state.epoch + 1
            torch.save(state, fp)

        # on peut visualiser avec
        # tensorboard --logdir runs/
        writer.add_scalar('Loss/train', cumul_loss/count_batch, n_iter)

        # Sortie directe
        print(f"Itérations {n_iter}: loss {cumul_loss/count_batch}")
        rec_loss_train[n_iter]= cumul_loss/count_batch


        #Evalute loss in test
        with torch.no_grad():
            for X_test,_ in data_test:
                #X_test = X_test.to(device)
                X_hat_test = state.model.cpu().forward(X_test)
                loss_test = loss_func(X_hat_test, X_test)
                state.model.to(device)
                writer.add_scalar('Loss/test', loss_test, n_iter)
                rec_loss_test[n_iter]=loss_test

        
    
    return rec_loss_train, rec_loss_test



if __name__ == "__main__":
    

    # Téléchargement des données
    ds = prepare_dataset("com.lecun.mnist");
    train_images, train_labels = ds.train.images.data(), ds.train.labels.data()
    test_images, test_labels =  ds.test.images.data(), ds.test.labels.data()

    BATCH_SIZE=32
    data_train = DataLoader(MonDataset(train_images, train_labels), shuffle=True, batch_size=BATCH_SIZE)
    data_test = DataLoader(MonDataset(test_images, test_labels), batch_size=test_images.shape[0])

    #Dimension à la quelle en réduit image
    # Exemple passage de 28*28=784 ---> 100
    dim_in = train_images.shape[1]*train_images.shape[2]
    dim_out = 100  
    model = Autoencodeur(dim_in, dim_out)

    #permet de  selectionner  le  gpu  si  disponible
    device = torch.device('cuda' if  torch.cuda.is_available() else 'cpu' )
    #chargement du module(des parametres) sur device
    model = model.to(device)

    #Optimiser les parametres model
    #descente_gradient(device, model, data_train, data_test, epsilon=0.3, epoch=500)

    # Pour visualiser
    with savepath.open("rb") as fp:
        state = torch.load(fp)
    for X_test,_ in data_test:
        """
        image_target = torch.tensor(X_test[0]).reshape(28,28)
        plt.imshow(np.clip(image_target, 0, 1))
        plt.show()
        """
        X = X_test[0].to(device)
        X_hat = state.model.forward(X).cpu()
        image_autoencoder = torch.tensor(X_hat).reshape(28,28)
        plt.imshow(np.clip(image_autoencoder, 0, 1))
        plt.show()
        
        
        break
        
